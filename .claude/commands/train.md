# Train Model with Unsloth

Start fine-tuning a model using Unsloth with configured LoRA adapters.

## Usage



## Arguments

- : Path to prepared training dataset
- : Base model name (default: llama-3.2-1b)
- : Number of training epochs (default: 3)
- : Training batch size (default: 4)
- : Learning rate (default: 2e-4)

## Examples



## Training Features

- 4-bit quantization for efficient training
- Gradient checkpointing
- Mixed precision training
- Automatic checkpoint saving
- Training loss monitoring

---
*Unsloth Fine-tuning Module*
