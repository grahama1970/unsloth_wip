# Complete Unsloth Training Pipeline Guide

## Overview

This guide documents the complete pipeline for training LoRA adapters with student-teacher thinking enhancement, from data generation to model deployment.

## Pipeline Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│    ArangoDB     │────▶│ Student-Teacher  │────▶│    Training     │
│  Q&A Generator  │     │    Enhancement   │     │  (Local/RunPod) │
└─────────────────┘     └──────────────────┘     └─────────────────┘
                                                           │
                                                           ▼
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│   Hugging Face  │◀────│   Validation     │◀────│   LoRA Adapter  │
│       Hub       │     │     Testing      │     │     Output      │
└─────────────────┘     └──────────────────┘     └─────────────────┘
```

## Step 1: Data Generation (ArangoDB Module)

The pipeline starts with Q&A data generated by the ArangoDB module:

```bash
# In the arangodb project
python generate_qa_data.py \
    --database your_db \
    --output /path/to/qa_output/qa_unsloth_20241223.jsonl
```

Output format:
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is machine learning?"},
    {"role": "assistant", "content": "Machine learning is..."}
  ],
  "metadata": {
    "thinking": "To answer this question, I need to explain...",
    "source": "document_123"
  }
}
```

## Step 2: Student-Teacher Enhancement

The key innovation: using the target model as the student to capture model-specific reasoning patterns.

### Configuration

```python
from src.unsloth.data.thinking_enhancer import StudentTeacherConfig

config = StudentTeacherConfig(
    # student_model automatically set to training model
    teacher_model="anthropic/max",  # Claude provides hints
    judge_model="gpt-4o-mini",      # Verifies correctness
    max_iterations=3,               # Up to 3 attempts
    thinking_format="iterative"     # Shows clear iterations
)
```

### How It Works

1. **Student Attempt**: The model we're training tries to answer
2. **Judge Check**: Verifies if the answer is correct
3. **Teacher Hint**: If wrong, Claude provides a pedagogical hint
4. **Iteration**: Student incorporates hint and tries again

Example enhancement:
```
Attempt 1:
Student (Phi-3.5): 15 * 13 = 15 + 15 + ... = 180
Judge: Incorrect

Teacher (Claude): Aha! Try breaking it down: 15 * 13 = 15 * (10 + 3)

Attempt 2:
Student: Oh wait, 15 * 10 = 150, and 15 * 3 = 45, so 150 + 45 = 195
Judge: Correct ✓
```

## Step 3: Training Configuration

### For Local Training (≤13B models)

```python
from src.unsloth.core.enhanced_config import EnhancedTrainingConfig

config = EnhancedTrainingConfig(
    model_name="unsloth/Phi-3.5-mini-instruct",
    max_seq_length=2048,
    
    # LoRA settings
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"],
    
    # Training
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    
    # Memory optimization
    gradient_checkpointing=True,
    load_in_4bit=True
)
```

### For RunPod Training (≥30B models)

The pipeline automatically detects when RunPod is needed based on:
- Model size (30B+, 70B+)
- Available GPU memory

RunPod configuration adds:
```python
pod_name="unsloth-llama-70b"
preferred_gpus=["H100 PCIe", "H100 SXM"]
use_flash_attention_2=True
```

## Step 4: Running the Complete Pipeline

### Simple Command

```bash
python -m src.unsloth.pipeline.complete_training_pipeline \
    --model unsloth/Phi-3.5-mini-instruct \
    --dataset /path/to/qa_data.jsonl \
    --output ./outputs/my_training \
    --hub-id myusername/phi-3.5-enhanced
```

### What Happens

1. **Auto-detection**: Determines if local or RunPod training is needed
2. **Enhancement**: Applies student-teacher thinking enhancement
3. **Training**: Trains LoRA adapter with optimizations
4. **Validation**: Tests the trained model
5. **Upload**: Pushes to Hugging Face Hub

### Pipeline Output

```
outputs/my_training/
├── enhanced_dataset.jsonl      # Enhanced Q&A data
├── checkpoints/               # Training checkpoints
├── adapter/                   # Final LoRA adapter
├── tensorboard/              # Training logs
└── pipeline_results.json     # Complete results
```

## Step 5: Validation

The pipeline automatically validates:

1. **Basic Inference**: Can the model generate text?
2. **Test Prompts**: Response quality on specific prompts
3. **Base Comparison**: How does it differ from base model?
4. **Performance**: Tokens/second, memory usage
5. **File Integrity**: All required files present

Example validation results:
```json
{
  "status": "passed",
  "tests": {
    "basic_inference": {"passed": true, "response_length": 127},
    "prompt_responses": [
      {
        "prompt": "What is the capital of France?",
        "response": "The capital of France is Paris...",
        "passed": true
      }
    ],
    "performance": {
      "tokens_per_second": 42.3,
      "gpu_memory_used_gb": 5.2
    }
  }
}
```

## Step 6: Model Usage

### Loading the Trained Adapter

```python
from unsloth import FastLanguageModel

# Load base model + adapter
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Phi-3.5-mini-instruct",
    adapter_name="myusername/phi-3.5-enhanced",
    max_seq_length=2048,
    load_in_4bit=True
)

# Enable fast inference
FastLanguageModel.for_inference(model)

# Generate
response = model.generate(
    tokenizer("Explain quantum computing", return_tensors="pt"),
    max_new_tokens=200
)
```

## Advanced Features

### 1. Grokking Support

Enable for superior generalization through extended training:

```python
from src.unsloth.core.grokking_config import GrokkingConfig

grokking = GrokkingConfig(
    enable_grokking=True,
    grokking_multiplier=30.0,  # 30x more epochs
    grokking_weight_decay=0.1  # Higher weight decay
)
```

### 2. Memory Optimization

For large models on limited GPUs:
- 4-bit quantization (automatic)
- Gradient checkpointing
- CPU offloading (if needed)
- Flash Attention 2 (on RunPod)

### 3. Monitoring

- TensorBoard: `tensorboard --logdir outputs/tensorboard`
- Progress tracking in `pipeline_results.json`
- Real-time logs with loguru

## Best Practices

1. **Data Quality**: Start with high-quality Q&A from ArangoDB
2. **Enhancement**: Always use student-teacher enhancement
3. **Model Selection**: Choose appropriate size for your GPU
4. **Validation**: Always validate before deployment
5. **Documentation**: Update model cards on Hugging Face

## Troubleshooting

### Out of Memory

- Reduce `per_device_train_batch_size`
- Increase `gradient_accumulation_steps`
- Enable `gradient_checkpointing`
- Use RunPod for larger models

### Slow Training

- Check if using 4-bit quantization
- Ensure Flash Attention is enabled
- Consider reducing `max_seq_length`

### Poor Results

- Increase training epochs
- Check enhancement quality
- Verify dataset size (need 1000+ examples)
- Try different learning rates

## Environment Setup

```bash
# Create environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export HF_TOKEN="your_huggingface_token"
export ANTHROPIC_API_KEY="your_claude_key"
export OPENAI_API_KEY="your_openai_key"  # For judge
export RUNPOD_API_KEY="your_runpod_key"  # Optional
```

## Example End-to-End Script

```bash
#!/bin/bash
# train_model.sh

# Configuration
MODEL="unsloth/Phi-3.5-mini-instruct"
DATASET="/data/qa_unsloth_latest.jsonl"
OUTPUT="./outputs/phi35_student_teacher"
HUB_ID="myorg/phi-3.5-enhanced-v1"

# Run pipeline
python -m src.unsloth.pipeline.complete_training_pipeline \
    --model $MODEL \
    --dataset $DATASET \
    --output $OUTPUT \
    --hub-id $HUB_ID

# Check results
cat $OUTPUT/pipeline_results.json | jq .status
```

## Next Steps

1. **Scale Up**: Try larger models with RunPod
2. **Custom Datasets**: Adapt for your specific domain
3. **Fine-tune Enhancement**: Adjust student-teacher parameters
4. **Production Deploy**: Use trained models in applications

## Support

- Issues: Create issues in the project repository
- Documentation: See individual module docs
- Examples: Check `src/unsloth/examples/`

This pipeline represents a complete solution for creating high-quality LoRA adapters with enhanced reasoning capabilities through student-teacher learning.